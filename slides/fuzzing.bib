@article{miller1990,
author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
title = {An empirical study of the reliability of UNIX utilities},
year = {1990},
issue_date = {Dec. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/96267.96279},
doi = {10.1145/96267.96279},
abstract = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
journal = {Commun. ACM},
month = {dec},
pages = {32–44},
numpages = {13}
},
@misc{miller1988,
title={CS736 Project List},
url={http://pages.cs.wisc.edu/~bart/fuzz/CS736-Projects-f1988.pdf},
author={Miller, Barton P.}, 
year={1988}
},
@misc{Dinaburg_2018, title={Fuzzing Like It's 1989},url={https://blog.trailofbits.com/2018/12/31/fuzzing-like-its-1989/}, journal={Trail of Bits Blog}, author={Dinaburg, Artem}, year={2018}, month={Dec}},
@incollection{fuzzingbook2024:Fuzzer,
    author = {Andreas Zeller and Rahul Gopinath and Marcel B{\"o}hme and Gordon Fraser and Christian Holler},
    booktitle = {The Fuzzing Book},
    title = {Fuzzing: Breaking Things with Random Inputs},
    year = {2024},
    publisher = {CISPA Helmholtz Center for Information Security},
    howpublished = {\url{https://www.fuzzingbook.org/html/Fuzzer.html}},
    note = {Retrieved 2024-01-18 18:11:45+01:00},
    url = {https://www.fuzzingbook.org/html/Fuzzer.html},
    urldate = {2024-01-18 18:11:45+01:00}
},
@misc{afl,
	title = {american fuzzy lop - a security oriented fuzzer},
	author = {Micha{\l} Zalewski},
	year = {2013}, 
	url = {https://github.com/google/AFL}
},
@article{dissecting_afl,
author = {Fioraldi, Andrea and Mantovani, Alessandro and Maier, Dominik and Balzarotti, Davide},
title = {Dissecting American Fuzzy Lop: A FuzzBench Evaluation},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3580596},
doi = {10.1145/3580596},
abstract = {AFL is one of the most used and extended fuzzers, adopted by industry and academic researchers alike. Although the community agrees on AFL’s effectiveness at discovering new vulnerabilities and its outstanding usability, many of its internal design choices remain untested to date. Security practitioners often clone the project “as-is” and use it as a starting point to develop new techniques, usually taking everything under the hood for granted. Instead, we believe that a careful analysis of the different parameters could help modern fuzzers improve their performance and explain how each choice can affect the outcome of security testing, either negatively or positively. The goal of this work is to provide a comprehensive understanding of the internal mechanisms of AFL by performing experiments and by comparing different metrics used to evaluate fuzzers. This can help to show the effectiveness of some techniques and to clarify which aspects are instead outdated. To perform our study, we performed nine unique experiments that we carried out on the popular Fuzzbench platform. Each test focuses on a different aspect of AFL, ranging from its mutation approach to the feedback encoding scheme and its scheduling methodologies. Our findings show that each design choice affects different factors of AFL. Some of these are positively correlated with the number of detected bugs or the coverage of the target application, whereas other features are related to usability and reliability. Most important, we believe that the outcome of our experiments indicates which parts of AFL we should preserve in the design of modern fuzzers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {52},
numpages = {26},
keywords = {Fuzzing, AFL, FuzzBench}
},
@ARTICLE{8863940,
  author={Manès, Valentin J.M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
  journal={IEEE Transactions on Software Engineering}, 
  title={The Art, Science, and Engineering of Fuzzing: A Survey}, 
  year={2021},
  volume={47},
  number={11},
  pages={2312-2331},
  keywords={Fuzzing;Security;Computer bugs;Terminology;Software security;automated software testing;fuzzing;fuzz testing},
  doi={10.1109/TSE.2019.2946563}},
@misc{sanitizers,
title = {sanitizers},
author = {Google Inc.},
year = {2012}, 
url = {https://github.com/google/sanitizers}},
@misc{githubseclab,
	title={Fuzzing software: advanced tricks (Part 2)},
	author = {Antonio Morales},
	journal = {GitHub Security Lab Blog},
	year = {2020},
	month = {jul},
	url = {https://securitylab.github.com/research/fuzzing-software-2/}
},
@misc{afl_asannotes,
	title = {notes for asan},
	author = {Micha{\l} Zalewski},
	year = {2013},
	url = {https://github.com/google/AFL/blob/master/docs/notes_for_asan.txt}
},
@inproceedings{klees,
author = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
title = {Evaluating Fuzz Testing},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243804},
doi = {10.1145/3243734.3243804},
abstract = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2123–2138},
numpages = {16},
keywords = {evaluation, fuzzing, security},
location = {Toronto, Canada},
series = {CCS '18}
},
@misc{aflcov,
	title = {afl-cov - AFL Fuzzing Code Coverage},
	author = {Michael Rash},
	url = {https://github.com/mrash/afl-cov},
       year = {2015}	
},
@INPROCEEDINGS{angora,
  author={Chen, Peng and Chen, Hao},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)}, 
  title={Angora: Efficient Fuzzing by Principled Search}, 
  year={2018},
  volume={},
  number={},
  pages={711-725},
  keywords={Computer bugs;Instruments;Fuzzing;Software;Space exploration;Shape;Measurement;vulnrability detection;coverage based fuzzing;taint analysis},
  doi={10.1109/SP.2018.00046}},
@INPROCEEDINGS{ijon,
  author={Aschermann, Cornelius and Schumilo, Sergej and Abbasi, Ali and Holz, Thorsten},
  booktitle={2020 IEEE Symposium on Security and Privacy (SP)}, 
  title={Ijon: Exploring Deep State Spaces via Fuzzing}, 
  year={2020},
  volume={},
  number={},
  pages={1597-1612},
  keywords={Fuzzing;Computer bugs;Space exploration;Software;Tools;Games;Instruments},
  doi={10.1109/SP40000.2020.00117}},
 @inproceedings{redqueen,
  title={REDQUEEN: Fuzzing with Input-to-State Correspondence},
  author={Aschermann, Cornelius and Schumilo, Sergej and Blazytko, Tim and Gawlik, Robert and Holz, Thorsten},
  booktitle={Symposium on Network and Distributed System Security (NDSS)},
  year={2019},
},
@misc{libafl,
	title = {The LibAFL Fuzzing Library},
	author = {Andrea Fioraldi and Dominik Maier},
	url = {https://aflplus.plus/libafl-book/libafl.html},
	year = {2024}
},
@inproceedings {h26forge,
author = {Willy R. Vasquez and Stephen Checkoway and Hovav Shacham},
title = {The Most Dangerous Codec in the World: Finding and Exploiting Vulnerabilities in H.264 Decoders},
booktitle = {32nd USENIX Security Symposium (USENIX Security 23)},
year = {2023},
isbn = {978-1-939133-37-3},
address = {Anaheim, CA},
pages = {6647--6664},
url = {https://www.usenix.org/conference/usenixsecurity23/presentation/vasquez},
publisher = {USENIX Association},
month = aug
},
@article{10.1145/1993316.1993532,
author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
title = {Finding and understanding bugs in C compilers},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993532},
doi = {10.1145/1993316.1993532},
abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {283–294},
numpages = {12},
keywords = {random testing, random program generation, compiler testing, compiler defect, automated testing}
},
@inproceedings{csmith,
author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
title = {Finding and understanding bugs in C compilers},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993532},
doi = {10.1145/1993498.1993532},
abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {283–294},
numpages = {12},
keywords = {random testing, random program generation, compiler testing, compiler defect, automated testing},
location = {San Jose, California, USA},
series = {PLDI '11}
},
@inproceedings{fuzzilli,
  title={FUZZILLI: Fuzzing for JavaScript JIT Compiler Vulnerabilities},
  author={Gro{\ss}, Samuel and Koch, Simon and Bernhard, Lukas and Holz, Thorsten and Johns, Martin},
  booktitle={Network and Distributed Systems Security (NDSS) Symposium},
  year={2023}
},
@inproceedings {langfuzz,
author = {Christian Holler and Kim Herzig and Andreas Zeller},
title = {Fuzzing with Code Fragments},
booktitle = {21st USENIX Security Symposium (USENIX Security 12)},
year = {2012},
isbn = {978-931971-95-9},
address = {Bellevue, WA},
pages = {445--458},
url = {https://www.usenix.org/conference/usenixsecurity12/technical-sessions/presentation/holler},
publisher = {USENIX Association},
month = aug
},
@inproceedings{grammarinator,
author = {Hodov\'{a}n, Ren\'{a}ta and Kiss, \'{A}kos and Gyim\'{o}thy, Tibor},
title = {Grammarinator: a grammar-based open source fuzzer},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278193},
doi = {10.1145/3278186.3278193},
abstract = {Fuzzing, or random testing, is an increasingly popular testing technique. The power of the approach lies in its ability to generate a large number of useful test cases without consuming expensive manpower. Furthermore, because of the randomness, it can often produce unusual cases that would be beyond the awareness of a human tester. In this paper, we present Grammarinator, a general purpose test generator tool that is able to utilize existing parser grammars as models. Since the model can act both as a parser and as a generator, the tool can provide the capabilities of both generation and mutation-based fuzzers. The presented tool is actively used to test various JavaScript engines and has found more than 100 unique issues.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {45–48},
numpages = {4},
keywords = {security, random testing, grammars, fuzzing},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
},
@inproceedings{aflfast,
author = {B\"{o}hme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik},
title = {Coverage-based Greybox Fuzzing as Markov Chain},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978428},
doi = {10.1145/2976749.2978428},
abstract = {Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few "high-frequency" paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule.We implemented the exponential schedule by extending AFL. In 24 hours, AFLFAST exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFAST produces at least an order of magnitude more unique crashes than AFL.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1032–1043},
numpages = {12},
keywords = {foundations, fuzzing, software security, testing efficiency, vulnerability detection},
location = {Vienna, Austria},
series = {CCS '16}
},
@inproceedings{aflgo,
author = {B\"{o}hme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
title = {Directed Greybox Fuzzing},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134020},
doi = {10.1145/3133956.3134020},
abstract = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into Google's continuous fuzzing platform OSS-Fuzz. Due to its directedness, AFLGo could find 39 bugs in several well-fuzzed, security-critical projects like LibXML2. 17 CVEs were assigned.},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2329–2344},
numpages = {16},
keywords = {verifying true positives, reachability, patch testing, directed testing, crash reproduction, coverage-based greybox fuzzing},
location = {Dallas, Texas, USA},
series = {CCS '17}
},
@article{sage,
    author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
    title = {SAGE: Whitebox Fuzzing for Security Testing: SAGE Has Had a Remarkable Impact at Microsoft.},
    year = {2012},
    issue_date = {January 2012},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {10},
    number = {1},
    issn = {1542-7730},
    url = {https://doi.org/10.1145/2090147.2094081},
    doi = {10.1145/2090147.2094081},
    journal = {Queue},
    month = {jan},
    pages = {20–27},
    numpages = {8}
},
@article{exe,
author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M. and Dill, David L. and Engler, Dawson R.},
title = {EXE: Automatically Generating Inputs of Death},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1094-9224},
url = {https://doi.org/10.1145/1455518.1455522},
doi = {10.1145/1455518.1455522},
abstract = {This article presents EXE, an effective bug-finding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, EXE runs it on symbolic input initially allowed to be anything. As checked code runs, EXE tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, EXE does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression, EXE forks execution, constraining the expression to be true on the true branch and false on the other. Because EXE reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug. When a path terminates or hits a bug, EXE automatically generates a test case by solving the current path constraints to find concrete values using its own co-designed constraint solver, STP. Because EXE’s constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deterministic code).EXE works well on real code, finding bugs along with inputs that trigger them in: the BSD and Linux packet filter implementations, the dhcpd DHCP server, the pcre regular expression library, and three Linux file systems.},
journal = {ACM Trans. Inf. Syst. Secur.},
month = {dec},
articleno = {10},
numpages = {38},
keywords = {test case generation, symbolic execution, dynamic analysis, constraint solving, bug finding, attack generation}
},
@article{10.1145/1064978.1065036,
author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
title = {DART: directed automated random testing},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1064978.1065036},
doi = {10.1145/1064978.1065036},
abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {213–223},
numpages = {11},
keywords = {automated test generation, interfaces, program verification, random testing, software testing}
},
@inproceedings{dart,
author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
title = {DART: directed automated random testing},
year = {2005},
isbn = {1595930566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1065010.1065036},
doi = {10.1145/1065010.1065036},
abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {213–223},
numpages = {11},
keywords = {automated test generation, interfaces, program verification, random testing, software testing},
location = {Chicago, IL, USA},
series = {PLDI '05}
},
@article{10.1145/1095430.1081750,
author = {Sen, Koushik and Marinov, Darko and Agha, Gul},
title = {CUTE: a concolic unit testing engine for C},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1081750},
doi = {10.1145/1095430.1081750},
abstract = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more specifically, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an efficient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {263–272},
numpages = {10},
keywords = {concolic testing, data structure testing, explicit path model-checking, random testing, testing C programs, unit testing}
},
@inproceedings{cute,
author = {Sen, Koushik and Marinov, Darko and Agha, Gul},
title = {CUTE: a concolic unit testing engine for C},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081750},
doi = {10.1145/1081706.1081750},
abstract = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more specifically, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an efficient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {263–272},
numpages = {10},
keywords = {concolic testing, data structure testing, explicit path model-checking, random testing, testing C programs, unit testing},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
},
@inproceedings{driller,
  title={Driller: Augmenting fuzzing through selective symbolic execution.},
  author={Stephens, Nick and Grosen, John and Salls, Christopher and Dutcher, Andrew and Wang, Ruoyu and Corbetta, Jacopo and Shoshitaishvili, Yan and Kruegel, Christopher and Vigna, Giovanni},
  booktitle={NDSS},
  volume={16},
  pages={1--16},
  year={2016}
},
@INPROCEEDINGS{mayhem,
  author={Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
  booktitle={2012 IEEE Symposium on Security and Privacy}, 
  title={Unleashing Mayhem on Binary Code}, 
  year={2012},
  volume={},
  number={},
  pages={380-394},
  keywords={Concrete;Computer bugs;Engines;Servers;Binary codes;Switches;Memory management;hybrid execution;symbolic memory;index-based memory modeling;exploit generation},
  doi={10.1109/SP.2012.31}},
 @inproceedings{gollum,
author = {Heelan, Sean and Melham, Tom and Kroening, Daniel},
title = {Gollum: Modular and Greybox Exploit Generation for Heap Overflows in Interpreters},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3354224},
doi = {10.1145/3319535.3354224},
abstract = {We present the first approach to automatic exploit generation for heap overflows in interpreters. It is also the first approach to exploit generation in any class of program that integrates a solution for automatic heap layout manipulation. At the core of the approach is a novel method for discovering exploit primitives---inputs to the target program that result in a sensitive operation, such as a function call or a memory write, utilizing attacker-injected data. To produce an exploit primitive from a heap overflow vulnerability, one has to discover a target data structure to corrupt, ensure an instance of that data structure is adjacent to the source of the overflow on the heap, and ensure that the post-overflow corrupted data is used in a manner desired by the attacker. Our system addresses all three tasks in an automatic, greybox, and modular manner. Our implementation is called GOLLUM, and we demonstrate its capabilities by producing exploits from 10 unique vulnerabilities in the PHP and Python interpreters, 5 of which do not have existing public exploits.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1689–1706},
numpages = {18},
keywords = {exploit generation, greybox, primitive search},
location = {London, United Kingdom},
series = {CCS '19}
},
@article{fuzzfactory,
author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Simon, Laurent and Vijayakumar, Hayawardh},
title = {FuzzFactory: domain-specific fuzzing with waypoints},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360600},
doi = {10.1145/3360600},
abstract = {Coverage-guided fuzz testing has gained prominence as a highly effective method of finding security vulnerabilities such as buffer overflows in programs that parse binary data. Recently, researchers have introduced various specializations to the coverage-guided fuzzing algorithm for different domain-specific testing goals, such as finding performance bottlenecks, generating valid inputs, handling magic-byte comparisons, etc. Each such solution can require non-trivial implementation effort and produces a distinct variant of a fuzzing tool. We observe that many of these domain-specific solutions follow a common solution pattern.  In this paper, we present FuzzFactory, a framework for developing domain-specific fuzzing applications without requiring changes to mutation and search heuristics. FuzzFactory allows users to specify the collection of dynamic domain-specific feedback during test execution, as well as how such feedback should be aggregated. FuzzFactory uses this information to selectively save intermediate inputs, called waypoints, to augment coverage-guided fuzzing. Such waypoints always make progress towards domain-specific multi-dimensional objectives. We instantiate six domain-specific fuzzing applications using FuzzFactory: three re-implementations of prior work and three novel solutions, and evaluate their effectiveness on benchmarks from Google's fuzzer test suite. We also show how multiple domains can be composed to perform better than the sum of their parts. For example, we combine domain-specific feedback about strict equality comparisons and dynamic memory allocations, to enable the automatic generation of LZ4 bombs and PNG bombs.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {174},
numpages = {29},
keywords = {domain-specific fuzzing, frameworks, fuzz testing, waypoints}
},
@misc{clusterfuzz,
title={ClusterFuzz},
author={Google Inc.},
url={https://google.github.io/clusterfuzz/},
year={2019},
month={feb}},
@inproceedings {enfuzz,
author = {Yuanliang Chen and Yu Jiang and Fuchen Ma and Jie Liang and Mingzhe Wang and Chijin Zhou and Xun Jiao and Zhuo Su},
title = {{EnFuzz}: Ensemble Fuzzing with Seed Synchronization among Diverse Fuzzers},
booktitle = {28th USENIX Security Symposium (USENIX Security 19)},
year = {2019},
isbn = {978-1-939133-06-9},
address = {Santa Clara, CA},
pages = {1967--1983},
url = {https://www.usenix.org/conference/usenixsecurity19/presentation/chen-yuanliang},
publisher = {USENIX Association},
month = aug
},
@inproceedings{collabfuzz,
author = {\"{O}sterlund, Sebastian and Geretto, Elia and Jemmett, Andrea and G\"{u}ler, Emre and G\"{o}rz, Philipp and Holz, Thorsten and Giuffrida, Cristiano and Bos, Herbert},
title = {CollabFuzz: A Framework for Collaborative Fuzzing},
year = {2021},
isbn = {9781450383370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447852.3458720},
doi = {10.1145/3447852.3458720},
abstract = {In the recent past, there has been lots of work on improving fuzz testing. In prior work, EnFuzz showed that by sharing progress among different fuzzers, they can perform better than the sum of their parts. In this paper, we continue this line of work and present CollabFuzz, a collaborative fuzzing framework allowing multiple different fuzzers to collaborate under an informed scheduling policy based on a number of central analyses. More specifically, CollabFuzz is a generic framework that allows a user to express different test case scheduling policies, such as the collaborative approach presented by EnFuzz. CollabFuzz can control which tests cases are handed out to what fuzzer and allows the orchestration of different fuzzers across the network. Furthermore, it allows the centralized analysis of the test cases generated by the various fuzzers under its control, allowing to implement scheduling policies based on the results of arbitrary program (e.g., data-flow) analysis.},
booktitle = {Proceedings of the 14th European Workshop on Systems Security},
pages = {1–7},
numpages = {7},
keywords = {automated bug finding, collaborative fuzzing, ensemble fuzzing, fuzzing, parallel fuzzing},
location = {Online, United Kingdom},
series = {EuroSec '21}
},
@inproceedings{fuzzbench,
  title={Fuzzbench: an open fuzzer benchmarking platform and service},
  author={Metzman, Jonathan and Szekeres, L{\'a}szl{\'o} and Simon, Laurent and Sprabery, Read and Arya, Abhishek},
  booktitle={Proceedings of the 29th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering},
  pages={1393--1403},
  year={2021}
},
@misc{macnair,
	title = {Fuzzing with AFL workshop},
	author = {Michael Macnair},
	year = {2017},
	url = {https://github.com/mykter/afl-training}
},
@inproceedings{seedselection,
author = {Herrera, Adrian and Gunadi, Hendra and Magrath, Shane and Norrish, Michael and Payer, Mathias and Hosking, Antony L.},
title = {Seed selection for successful fuzzing},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464795},
doi = {10.1145/3460319.3464795},
abstract = {Mutation-based greybox fuzzing---unquestionably the most widely-used fuzzing technique---relies on a set of non-crashing seed inputs (a corpus) to bootstrap the bug-finding process. When evaluating a fuzzer, common approaches for constructing this corpus include: (i) using an empty file; (ii) using a single seed representative of the target's input format; or (iii) collecting a large number of seeds (e.g., by crawling the Internet). Little thought is given to how this seed choice affects the fuzzing process, and there is no consensus on which approach is best (or even if a best approach exists).  To address this gap in knowledge, we systematically investigate and evaluate how seed selection affects a fuzzer's ability to find bugs in real-world software. This includes a systematic review of seed selection practices used in both evaluation and deployment contexts, and a large-scale empirical evaluation (over 33 CPU-years) of six seed selection approaches. These six seed selection approaches include three corpus minimization techniques (which select the smallest subset of seeds that trigger the same range of instrumentation data points as a full corpus).  Our results demonstrate that fuzzing outcomes vary significantly depending on the initial seeds used to bootstrap the fuzzer, with minimized corpora outperforming singleton, empty, and large (in the order of thousands of files) seed sets. Consequently, we encourage seed selection to be foremost in mind when evaluating/deploying fuzzers, and recommend that (a) seed choice be carefully considered and explicitly documented, and (b) never to evaluate fuzzers with only a single seed.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {230–243},
numpages = {14},
keywords = {software testing, fuzzing, corpus minimization},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}
